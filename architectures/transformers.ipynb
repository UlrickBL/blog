{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06aed046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a02a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2 - loss : 9.329459190368652\n",
      "Epoch 1/2 - loss : 9.200955390930176\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs :\n",
    "    n_layers : int\n",
    "    hidden_dim : int\n",
    "    head_dim : int\n",
    "    \n",
    "\n",
    "class SimpleSelfAttention(nn.Module) :\n",
    "    def __init__(self,hidden_dim) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.wq = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.wk = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.wv = nn.Linear(hidden_dim,hidden_dim)\n",
    "        \n",
    "        self.wo = nn.Linear(hidden_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        # x is of shape (B,S,d)\n",
    "        \n",
    "        xq = self.wq(x) # shape (B, S, d)\n",
    "        xk = self.wk(x)\n",
    "        xv = self.wv(x)\n",
    "        \n",
    "        attn = (xq @ xk.transpose(-1,-2))/(self.hidden_dim ** 0.5)\n",
    "        \n",
    "        attn_softmax = F.softmax(attn,dim=-1) # (B, S, S)\n",
    "        \n",
    "        out = attn_softmax @ xv # (B, S, d)\n",
    "        \n",
    "        return self.wo(out)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module) :\n",
    "    def __init__(self,hidden_dim,n_heads, head_dim) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "        self.wq = nn.Linear(hidden_dim,n_heads *head_dim)\n",
    "        self.wk = nn.Linear(hidden_dim,n_heads *head_dim)\n",
    "        self.wv = nn.Linear(hidden_dim,n_heads *head_dim)\n",
    "        \n",
    "        self.wo = nn.Linear(n_heads *head_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        # x is of shape (B,S,d)\n",
    "        B, S, d = x.shape\n",
    "        \n",
    "        xq = self.wq(x).view(B, S, self.n_heads, self.head_dim).transpose(1,2) # (B, n_heads, S, head_dim)\n",
    "        xk = self.wk(x).view(B, S, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        xv = self.wv(x).view(B, S, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        attn = (xq @ xk.transpose(-1,-2))/(self.hidden_dim ** 0.5) # (B, n_heads, S, S)\n",
    "        \n",
    "        attn_softmax = F.softmax(attn,dim=-1) # (B, n_heads, S, S)\n",
    "        \n",
    "        out = attn_softmax @ xv # (B, n_heads, S, head_dim)\n",
    "        \n",
    "        out = out.transpose(1,2).contiguous().view(B,S,self.hidden_dim)\n",
    "        \n",
    "        return self.wo(out)\n",
    "\n",
    "class MaskedMultiHeadSelfAttention(nn.Module) :\n",
    "    def __init__(self,hidden_dim,n_heads, head_dim) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "        self.wq = nn.Linear(hidden_dim,n_heads *head_dim)\n",
    "        self.wk = nn.Linear(hidden_dim,n_heads *head_dim)\n",
    "        self.wv = nn.Linear(hidden_dim,n_heads *head_dim)\n",
    "        \n",
    "        self.wo = nn.Linear(n_heads *head_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        # x is of shape (B,S,d)\n",
    "        B, S, d = x.shape\n",
    "        \n",
    "        xq = self.wq(x).view(B, S, self.n_heads, self.head_dim).transpose(1,2) # (B, n_heads, S, head_dim)\n",
    "        xk = self.wk(x).view(B, S, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        xv = self.wv(x).view(B, S, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        attn = (xq @ xk.transpose(-1,-2))/(self.head_dim ** 0.5) # (B, n_heads, S, S)\n",
    "        \n",
    "        mask = torch.triu(torch.ones(S,S,device=x.device),diagonal=1).unsqueeze(0).unsqueeze(0)\n",
    "        attn = attn.masked_fill(mask==1,-float(\"inf\"))\n",
    "        \n",
    "        attn_softmax = F.softmax(attn,dim=-1) # (B, n_heads, S, S)\n",
    "        \n",
    "        out = attn_softmax @ xv # (B, n_heads, S, head_dim)\n",
    "        \n",
    "        out = out.transpose(1,2).contiguous().view(B,S,self.hidden_dim)\n",
    "        \n",
    "        return self.wo(out)\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_q_heads, n_kv_heads, head_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        assert n_q_heads % n_kv_heads == 0\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_q_heads = n_q_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.group_size = n_q_heads // n_kv_heads\n",
    "\n",
    "        self.wq = nn.Linear(hidden_dim, n_q_heads * head_dim)\n",
    "        self.wk = nn.Linear(hidden_dim, n_kv_heads * head_dim)\n",
    "        self.wv = nn.Linear(hidden_dim, n_kv_heads * head_dim)\n",
    "\n",
    "        self.wo = nn.Linear(n_q_heads * head_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, _ = x.shape\n",
    "\n",
    "        q = self.wq(x).view(B, S, self.n_q_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(x).view(B, S, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(x).view(B, S, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Repeat K/V for grouped heads\n",
    "        k = k.repeat_interleave(self.group_size, dim=1)\n",
    "        v = v.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        attn = (q @ k.transpose(-1, -2)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        mask = torch.triu(torch.ones(S, S, device=x.device), diagonal=1)\n",
    "        attn = attn.masked_fill(mask.bool(), float(\"-inf\"))\n",
    "\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = attn @ v\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, S, -1)\n",
    "        return self.wo(out)\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self,hidden_dim,intermediate_dim) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w1 = nn.Linear(hidden_dim,intermediate_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim,intermediate_dim)\n",
    "        \n",
    "        self.wo = nn.Linear(intermediate_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        # x is of shape (B, S, d)\n",
    "        return self.wo(F.silu(self.w1(x)) * self.w2(x))\n",
    "\n",
    "class RMSNorm(nn.Module) :\n",
    "    def __init__(self,epsilon, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = nn.Parameter(torch.ones(hidden_dim))\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        # x / (norm x + eps)\n",
    "        norm_x = x.pow(2).mean(dim=-1,keepdim=True).sqrt()\n",
    "        #norm_x = torch.norm(x,dim=-1,keepdim=True)\n",
    "        return self.alpha * (x/(norm_x + self.epsilon))\n",
    "\n",
    "class TransformerBlock(nn.Module) :\n",
    "    def __init__(self,hidden_dim,intermediate_dim,n_heads, head_dim,epsilon) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.masked_multihead_attention = MaskedMultiHeadSelfAttention(hidden_dim,n_heads, head_dim)\n",
    "        self.swiglu = SwiGLU(hidden_dim,intermediate_dim)\n",
    "        \n",
    "        self.norm_1 = RMSNorm(epsilon, hidden_dim)\n",
    "        self.norm_2 = RMSNorm(epsilon, hidden_dim)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        x = x + self.masked_multihead_attention(self.norm_1(x))\n",
    "        x = x + self.swiglu(self.norm_2(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module) :\n",
    "    def __init__(self,hidden_dim,intermediate_dim,n_heads, head_dim,epsilon, n_layers, vocab_size) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.layers = nn.ModuleList(TransformerBlock(hidden_dim,intermediate_dim,n_heads, head_dim,epsilon) for _ in range(n_layers))\n",
    "        self.classifier = nn.Linear(hidden_dim,vocab_size)\n",
    "        \n",
    "    def forward(self,x) :\n",
    "        h = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers :\n",
    "            h = layer(h)\n",
    "        \n",
    "        out = self.classifier(h)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "model = Transformer(hidden_dim=128,intermediate_dim=256,n_heads=4, head_dim=32,epsilon=1e-8, n_layers=3, vocab_size=10000)\n",
    "model.train()\n",
    "\n",
    "batch_size = 32\n",
    "sequence_len = 24\n",
    "epochs = 2\n",
    "learning_rate = 10e-4\n",
    "vocab_size=10000\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "x = torch.randint(low=0,high=10000,size=(batch_size, sequence_len))\n",
    "y = torch.randint(low=0,high=10000,size=(batch_size, sequence_len))\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred.view(-1,vocab_size),y.view(-1).long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch}/{epochs} - loss : {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "with open(\"path.txt\",\"r\") as f :\n",
    "    text = f.read()\n",
    "    \n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "class TrainingDataset(Dataset) :\n",
    "    def __init__(self,tokens,sequence_len) :\n",
    "        super().__init__()\n",
    "        self.tokens = tokens\n",
    "        self.sequence_len = sequence_len\n",
    "    def __len__(self) :\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self,idx) :\n",
    "        x = torch.tensor(tokens[idx : idx+self.sequence_len])\n",
    "        y = torch.tensor(tokens[idx+1 : idx+self.sequence_len+1])\n",
    "        return x, y\n",
    "\n",
    "dataset = TrainingDataset(tokens,128)\n",
    "\n",
    "loader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    for x,y in loader :\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred.view(-1,vocab_size),y.view(-1).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}/{epochs} - loss : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,stride=2,padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=2,padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        self.classifier = nn.Linear(32*1*1,2)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        return self.classifier(x)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
